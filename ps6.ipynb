{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "23d9b33e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from collections import defaultdict\n",
    "import json\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b0cd088e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper functions\n",
    "# Utilities\n",
    "last_tree = None\n",
    "last_formula = None\n",
    "\n",
    "def parse_and_display(s, p, graphic:bool=True):\n",
    "   global last_tree, last_formula\n",
    "   for tree in p.parse(s.split()): \n",
    "    last_tree = tree\n",
    "    if graphic: \n",
    "        display(tree) \n",
    "    else: print(tree)\n",
    "    last_formula = last_tree.label()['SEM']\n",
    "    if graphic:\n",
    "        display(Markdown(f'${expr_to_latex(last_formula)}$'))\n",
    "    else: print(last_formula)\n",
    "\n",
    "def expr_to_latex(expr):\n",
    "    \"\"\"\n",
    "    Recursively converts an nltk.Expression to a LaTeX string.\n",
    "    \"\"\"\n",
    "    if isinstance(expr, nltk.sem.logic.ApplicationExpression):  # Function application (e.g., char(n, lett))\n",
    "        # Extract function (predicate) and arguments\n",
    "        arguments = []\n",
    "        while isinstance(expr, nltk.sem.logic.ApplicationExpression):\n",
    "            arguments.append(expr.argument)\n",
    "            expr = expr.function  # Move up to the function name\n",
    "        \n",
    "        # Now expr is the function name (predicate), and arguments contains all its arguments\n",
    "        function_name = rf\"\\mathbf{{{expr}}}\"  # Boldface the predicate name\n",
    "        arguments = \", \".join(expr_to_latex(arg) for arg in reversed(arguments))  # Reverse to maintain correct order\n",
    "        return f\"{function_name}({arguments})\"\n",
    "    elif isinstance(expr, nltk.sem.logic.LambdaExpression):  # Lambda abstraction (e.g., \\x.P(x))\n",
    "        return rf\"\\lambda {expr.variable} . {expr_to_latex(expr.term)}\"\n",
    "    elif isinstance(expr, nltk.sem.logic.QuantifiedExpression):  # Quantifiers (e.g., exists n. P(n))\n",
    "        quantifier = r\"\\forall\" if expr.getQuantifier() == \"all\" else r\"\\exists\"\n",
    "        return rf\"{quantifier} {expr.variable} \\, {expr_to_latex(expr.term)}\"\n",
    "    elif isinstance(expr, nltk.sem.logic.NegatedExpression):  # Negation (e.g., Â¬P)\n",
    "        return rf\"\\neg {expr_to_latex(expr.term)}\"\n",
    "    elif isinstance(expr, nltk.sem.logic.AndExpression):  # Conjunction (P & Q)\n",
    "        return rf\"({expr_to_latex(expr.first)} \\wedge {expr_to_latex(expr.second)})\"\n",
    "    elif isinstance(expr, nltk.sem.logic.OrExpression):  # Disjunction (P | Q)\n",
    "        return rf\"({expr_to_latex(expr.first)} \\vee {expr_to_latex(expr.second)})\"\n",
    "    elif isinstance(expr, nltk.sem.logic.ImpExpression):  # Implication (P -> Q)\n",
    "        return rf\"({expr_to_latex(expr.first)} \\rightarrow {expr_to_latex(expr.second)})\"\n",
    "    elif isinstance(expr, nltk.sem.logic.BinaryExpression):  # Handles '<->' equivalence\n",
    "        if expr.operator == '<->':\n",
    "            return rf\"({expr_to_latex(expr.first)} \\leftrightarrow {expr_to_latex(expr.second)})\"\n",
    "        else:\n",
    "            return rf\"({expr_to_latex(expr.first)} {expr.operator} {expr_to_latex(expr.second)})\"\n",
    "    elif isinstance(expr, nltk.sem.logic.IndividualVariableExpression):  # Variables (e.g., x, y, n)\n",
    "        return str(expr)\n",
    "    elif isinstance(expr, nltk.sem.logic.ConstantExpression):  # Constants (e.g., lett, numbers)\n",
    "        expr_str = str(expr)\n",
    "        if expr_str.isdigit():  # If it's a number, don't boldface it\n",
    "            return expr_str\n",
    "        return rf\"\\mathbf{{{expr_str}}}\"  # Boldface non-numeric constants like 'lett'\n",
    "    else:\n",
    "        return str(expr)  # Default case for any unhandled expression type\n",
    "# Authors chatgpt4o and Mats Rooth Feb 8 2025\n",
    "\n",
    "def emptysets(val:nltk.sem.evaluate.Valuation):\n",
    "  val.update([(k,set()) for (k,v) in val.items() if v == 'set()'])\n",
    "\n",
    "# Model construction\n",
    "from typing import Callable, List, Set\n",
    "\n",
    "def to_model_str(word: str, special_rels: List[Callable[[str], str]]=[]) -> str:\n",
    "    \"\"\"\n",
    "    Creates the string form of the model for the input word. This string is meant to be passed to `nltk.Valuation.fromstring`.\n",
    "    By default, the function will only add the relations mapping i => i for i from 1 to the length of `word` and a relation \n",
    "    mapping char => the set of tuples (i, word[i]). The `special_rels` function allows you to specify additional relations to \n",
    "    be added to the valuation string.\n",
    "    \n",
    "    :param word: The word to create a model string for.\n",
    "    :param special_rels: A list of functions that when called return a string of the form {relation_name} => {relation_contents}. Defaults to the empty list.\n",
    "    :returns: a string representing the model for word\n",
    "    \"\"\"\n",
    "    n = len(word)\n",
    "    model_str = []\n",
    "    char = []\n",
    "    for i in range(1, n+1):\n",
    "        model_str.append(f'{i} => {i}')\n",
    "        char.append((i, word[i-1]))\n",
    "    model_str.append(f'char => {set(char)}'.lower())\n",
    "    return '\\n'.join(model_str + [rel(word) for rel in special_rels]).replace(\"'\", \"\")\n",
    "# Angela Liu\n",
    "\n",
    "###This code is from CL1 2023\n",
    "\n",
    "vowels = ['a', 'e', 'i', 'o', 'u']\n",
    "fricatives = ['v', 'f', 's', 'z', 'h', 'th', 'sh', 'zh']\n",
    "\n",
    "def capital(word, i):\n",
    "    return word[i].isupper()\n",
    "\n",
    "get_capital = lambda word: f'capital => {set([i+1 for i in range(len(word)) if capital(word,i)])}'\n",
    "\n",
    "def less_than(i, j):\n",
    "    return i<j\n",
    "get_less_than = lambda word: f'le => {set([(i+1,j+1) for i in range(len(word)) for j in range(len(word)) if i!=j and less_than(i+1,j+1)])}'\n",
    "\n",
    "def adjacent(i,j):\n",
    "    return abs(i-j) == 1\n",
    "\n",
    "get_adjacent = lambda word: f'ad => {set([(i+1,j+1) for i in range(len(word)) for j in range(len(word)) if adjacent(i+1,j+1)])}'\n",
    "\n",
    "get_even =  lambda word: f'even => {set([i+1 for i in range(len(word)) if (i+1)%2==0])}'\n",
    "\n",
    "get_odd = lambda word: f'odd => {set([i+1 for i in range(len(word)) if (i+1)%2!=0])}'\n",
    "\n",
    "# @323\n",
    "def voiced(word):\n",
    "    word=word.lower()\n",
    "    v=[]\n",
    "    for i in range(len(word)):\n",
    "        if i != len(word)-1:\n",
    "            if (word[i] == 'n' and word[i+1]=='g') or (word[i] == 's' and word[i+1] == 'z'):\n",
    "                v.append((i+1, word[i]))\n",
    "                v.append((i+2, word[i+1]))\n",
    "        if word[i] == 'z':\n",
    "            if ((i,'s') not in v):\n",
    "                v.append((i+1, 'z'))\n",
    "        if word[i] == 'g':\n",
    "            if ((i,'n') not in v):\n",
    "                v.append((i+1, 'g'))\n",
    "        if word[i] == 'n':\n",
    "            if ((i+2,'g') not in v):\n",
    "                v.append((i+1,'n'))\n",
    "        if word[i] in ['a', 'e', 'i', 'o', 'u', 'b', 'd', 'j', 'l', 'm', 'r', 'v', 'w', 'y']:\n",
    "            v.append((i+1, word[i]))\n",
    "    return v\n",
    "            \n",
    "get_voiced= lambda w: f'voiced => {set([(i+1,w[i].lower()) for i in range(len(w)) if (i+1, w[i].lower()) in voiced(w)])}'\n",
    "\n",
    "def centered(word,i):\n",
    "    if (len(word)%2==0):\n",
    "        return len(word)//2 == i or i == len(word)//2 + 1\n",
    "    else:\n",
    "        return len(word)//2 + 1 == i\n",
    "\n",
    "get_centered = lambda word: f'cent => {set([i+1 for i in range(len(word)) if centered(word,i+1)])}'\n",
    "\n",
    "get_mirrored = lambda w: f'mirrored => {set(i+1 for i in range(len(w)) if w[i].lower() == w[len(w)-1-i].lower())}'\n",
    "\n",
    "get_glide =  lambda w: f'glide => {set(i+1 for i in range(len(w)) if w[i].lower() == \"w\" or w[i].lower() == \"y\")}'\n",
    "\n",
    "# from @325\n",
    "def fricatives(word):\n",
    "    word.lower()\n",
    "    frics = []\n",
    "    for i in range(len(word)):\n",
    "        if i != len(word)-1:\n",
    "            if word[i] in ['t', 's', 'z'] and word[i+1] == 'h':\n",
    "                frics.append((i+1, word[i]))\n",
    "                frics.append((i+2, 'h'))\n",
    "        if word[i] == 'h':\n",
    "            if ((i, 't') not in frics) and ((i, 's') not in frics) and ((i, 'z') not in frics):\n",
    "                frics.append((i+1, 'h'))\n",
    "        if word[i] in ['s', 'z']:\n",
    "            if ((i+2, 'h') not in frics):\n",
    "                frics.append((i+1, word[i]))\n",
    "        if word[i] in ['v', 'f']:\n",
    "            frics.append((i+1, word[i]))\n",
    "        return frics\n",
    "\n",
    "get_fricative = lambda w: f'fricative => {set([(i+1,w[i].lower()) for i in range(len(w)) if (i+1, w[i].lower()) in fricatives(w)])}'\n",
    "\n",
    "\n",
    "# get all the vowels in a word\n",
    "get_vowel = lambda w: f'vowel => {set(re.findall(r\"[AEIOUaeiou]\", w))}'.lower()\n",
    "\n",
    "# get all the consonants in a word\n",
    "get_cons = lambda w: 'consonant => {}'.format(set(re.findall(r\"[^AEIOUaeiouywYW\\W0-9]\", w))).lower()\n",
    "\n",
    "# get all the tuple of two numbers (n,m) such that n < m, n&m < len(word) and n!=m\n",
    "follows = lambda w: f'le => {set([(i+1,j+1) for i in range(len(w)) for j in range(i, len(w)) if i != j])}'\n",
    "\n",
    "# get all the letters that are capitalized\n",
    "get_capital = lambda w: 'capital => {}'.format(set(re.findall(r\"[A-Z]\",w))).lower()\n",
    "# for A[SEM=<\\n.exists c.(capital(n)& char(n,c))>] -> 'capitalized'\n",
    "\n",
    "# get_capital = lambda w: f'capital => {set([m.span()[0] + 1 for m in re.finditer(r\"[A-Z]\", w)])}'\n",
    "\n",
    "# get all the glides in a word\n",
    "get_glide = lambda w: f'glide => {set(re.findall(r\"[ywYW]\", w))}'.lower()\n",
    "\n",
    "# for exactly one\n",
    "equals = lambda w: f'eq => {set([(i+1,i+1) for i in range(len(w))])}'\n",
    "\n",
    "# get all alphabetical letters in a word\n",
    "get_alphabet = lambda w: f'alphabet => {set(re.findall(r\"[A-Za-z]\", w))}'.lower()\n",
    "\n",
    "# get all liquids\n",
    "get_liquid = lambda w: f'liquid => {set(re.findall(r\"[lrLR]\", w))}'.lower()\n",
    "\n",
    "# get all nasals\n",
    "get_nasal = lambda w: f'nasal => {set(re.findall(r\"[nmNM]\", w))}'.lower()\n",
    "\n",
    "# get all plosives\n",
    "get_plosive = lambda w: f'plosive => {set(re.findall(r\"[pbtdkgPBTDKG]\", w))}'.lower()\n",
    "\n",
    "\n",
    "\n",
    "letter_funcs = {\n",
    "    f\"let{ch}\": (lambda c: lambda w: f\"let{c} => {c}\")(ch)\n",
    "    for ch in string.ascii_lowercase\n",
    "}\n",
    "\n",
    "all_func = [\n",
    "    follows, get_capital, get_vowel, equals, get_alphabet,\n",
    "    get_adjacent, get_voiced, get_fricative, get_glide, get_centered,\n",
    "    get_mirrored, get_less_than, get_even, get_odd,\n",
    "    get_liquid, get_nasal, get_plosive, get_cons\n",
    "] + list(letter_funcs.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc15b667",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsing sentence: AT LEAST ONE VOWEL IS CAPITALIZED with ID: aiden-a-1-1\n",
      "Parsing sentence: THERE IS EXACTLY ONE T with ID: aiden-a-10-4\n",
      "Parsing sentence: SOME VOWELS IS NOT CAPITALIZED with ID: aiden-a-2-2\n",
      "Parsing sentence: SOME VOWEL IS NOT CAPITALIZED with ID: aiden-a-2-4\n",
      "Parsing sentence: SOME VOWELS IS NOT CAPITAL with ID: aiden-a-2-6\n",
      "Parsing sentence: SOME VOWEL IS NOT CAPITAL with ID: aiden-a-2-11\n",
      "Parsing sentence: EVERY GLIDE PRECEDES THE VOWEL with ID: aiden-a-3-3\n",
      "Parsing sentence: THE LETTER THAT PRECEDES LETTER FIVE IS A VOWEL with ID: aiden-a-4-241\n",
      "Parsing sentence: THE LETTER THAT PRECEDES THE LETTER FIVE IS A VOWEL with ID: aiden-a-4-288\n",
      "Parsing sentence: THE LETTER THAT PRECEDES A LETTER FIVE IS A VOWEL with ID: aiden-a-4-309\n",
      "Parsing sentence: THE LETTER THAT PRECEDES LETTER FIVE IS THE VOWEL with ID: aiden-a-4-444\n",
      "Parsing sentence: THE LETTER THAT PRECEDES THE LETTER FIVE IS THE VOWEL with ID: aiden-a-4-491\n",
      "Parsing sentence: THE LETTER THAT PRECEDES A LETTER FIVE IS THE VOWEL with ID: aiden-a-4-513\n",
      "Parsing sentence: LETTER THREE IS UNIQUE with ID: aiden-a-5-1\n",
      "Parsing sentence: THE LETTER THREE IS UNIQUE with ID: aiden-a-5-2\n",
      "Parsing sentence: A LETTER THREE IS UNIQUE with ID: aiden-a-5-3\n",
      "Parsing sentence: THE PENULTIMATE LETTER IS NOT A VOWEL with ID: aiden-a-6-1\n",
      "Parsing sentence: THE PENULTIMATE LETTER IS NOT THE VOWEL with ID: aiden-a-6-3\n",
      "Parsing sentence: NO CONSONANT IS UNIQUE with ID: aiden-a-7-1\n",
      "Parsing sentence: THERE IS AT LEAST ONE GLIDE with ID: aiden-a-8-4\n",
      "Parsing sentence: THERE IS AT LEAST ONE NOT GLIDE with ID: aiden-a-8-5\n",
      "Parsing sentence: ONLY LETTER TWO IS UNIQUE with ID: aiden-a-9-7\n"
     ]
    }
   ],
   "source": [
    "# Problem 1\n",
    "parsable_transcripts = defaultdict(list)\n",
    "parsable_formulas = defaultdict(list)\n",
    "\n",
    "parser = nltk.load_parser('ps6_grammar.fcfg', trace=0)\n",
    "count = 0\n",
    "with open(\"1000.txt\", \"r\") as f:\n",
    "    for line in f:\n",
    "        count += 1\n",
    "        if count % 5000 == 0:\n",
    "            print(f\"Processed {count} lines...\")\n",
    "        if line.strip():\n",
    "            parts = line.strip().split(maxsplit=1)\n",
    "            if len(parts) == 2:\n",
    "                id, sentence = parts\n",
    "                try:\n",
    "                    trees = parser.parse(sentence.lower().split())\n",
    "                    trees_list = list(trees)\n",
    "                    if (trees is not None) and (len(trees_list) > 0):\n",
    "                        parts = id.split('-')\n",
    "                        root_id = '-'.join(parts[:-1])\n",
    "                        parsable_transcripts[root_id].append(id)\n",
    "                        print(f\"Parsing sentence: {sentence} with ID: {id}\")\n",
    "                        parsable_formulas[root_id].append(trees_list[0].label()['SEM'])\n",
    "                    else:\n",
    "                        continue\n",
    "                except Exception as e:\n",
    "                    print(f\"Error parsing sentence '{sentence}': {e}\")\n",
    "            else:\n",
    "                print(\"Skipping line in text file:\", line)\n",
    "\n",
    "try:\n",
    "    with open(\"parable_transcripts.json\", \"r\") as f:\n",
    "        all_data = json.load(f)\n",
    "        if not isinstance(all_data, list):\n",
    "            all_data = []\n",
    "except FileNotFoundError:\n",
    "    all_data = []\n",
    "\n",
    "for root_id, transcript_ids in parsable_transcripts.items():\n",
    "    formulas = parsable_formulas[root_id]\n",
    "    formulas = (lambda x: [str(f) for f in x])(formulas)\n",
    "    \n",
    "    data_entry = {\n",
    "        \"root_id\": root_id,\n",
    "        \"transcript_ids\": transcript_ids,\n",
    "        \"formulas\": formulas\n",
    "    }\n",
    "    \n",
    "    all_data.append(data_entry)\n",
    "    \n",
    "# Save to individual JSON file\n",
    "with open(\"parable_transcripts.json\", \"w\") as f:\n",
    "    json.dump(all_data, f, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fec4926e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total words evaluated: 0\n",
      "Correct truth value predictions: 0\n",
      "Proportion of correct predictions: 0.00\n",
      "Total IDs evaluated: 0\n",
      "IDs with >= 85% correct predictions: 0\n",
      "Proportion of IDs with >= 85% correct predictions: 0.00\n"
     ]
    }
   ],
   "source": [
    "# Problem 2\n",
    "# Part A: Load the data from the JSON file\n",
    "try:\n",
    "    with open(\"parable_transcripts.json\", \"r\") as f:\n",
    "        all_data = json.load(f)\n",
    "except FileNotFoundError:\n",
    "    all_data = []\n",
    "    \n",
    "# Part B: Load the truth values\n",
    "truth_values = {}\n",
    "with open(\"pooled_truth\", \"r\") as f:\n",
    "    for line in f:\n",
    "        if line.strip():\n",
    "            parts = line.strip().lower().split()\n",
    "            if len(parts) >= 2:\n",
    "                id = parts[0]\n",
    "                # The rest are word-value pairs\n",
    "                word_values = []\n",
    "                for i in range(1, len(parts), 2):\n",
    "                    if i+1 < len(parts):\n",
    "                        word = parts[i]\n",
    "                        value = parts[i+1]\n",
    "                        if value in ['t', 'f']:  # Skip 's' and 'u' values\n",
    "                            word_values.append((word, value == 't'))\n",
    "                truth_values[id] = word_values\n",
    "  \n",
    "# Part B/C: Compute the ratios\n",
    "total_words = 0\n",
    "correct_truths = 0\n",
    "\n",
    "ids_over_85_percent = 0\n",
    "total_ids = 0\n",
    "\n",
    "for data_entry in all_data:\n",
    "    root_id = data_entry[\"root_id\"]\n",
    "    transcript_ids = data_entry[\"transcript_ids\"]\n",
    "    formulas = data_entry[\"formulas\"]\n",
    "    \n",
    "    transcript_formula_pairs = list(zip(transcript_ids, formulas))\n",
    "    transcript_formula_pairs.sort(key=lambda x: x[0])  # Sort by transcript ID\n",
    "    \n",
    "    for transcript_id, formula in transcript_formula_pairs:\n",
    "        if transcript_id in truth_values:\n",
    "            word_values = truth_values[transcript_id]\n",
    "            words = [w for w, v in word_values]\n",
    "            truths = [v for w, v in word_values]\n",
    "\n",
    "            vals = [nltk.Valuation.fromstring(to_model_str(w, all_func)) for w in words]\n",
    "            for val in vals: emptysets(val)\n",
    "            models = [nltk.Model(val.domain, val) for val in vals]\n",
    "            assignments = [nltk.Assignment(val.domain) for val in vals]\n",
    "            \n",
    "            num_correct = 0\n",
    "            total_truths = 0\n",
    "            total_ids += 1\n",
    "\n",
    "            for idx, truth in enumerate(truths):\n",
    "                total_words += 1\n",
    "                total_truths += 1\n",
    "                try:\n",
    "                    result = models[idx].evaluate(str(formula), assignments[idx])\n",
    "                    if result == truth:\n",
    "                        # If the evaluation matches the truth value, we can count it as correct\n",
    "                        correct_truths += 1\n",
    "                        num_correct += 1\n",
    "                except Exception as e:\n",
    "                    print(f\"Error evaluating transript_id {root_id}-{transcript_id}\")\n",
    "            \n",
    "            if total_truths > 0 and num_correct / total_truths >= 0.85:\n",
    "                ids_over_85_percent += 1\n",
    "                \n",
    "      \n",
    "# Proportion of truth value predictions that are correct\n",
    "print(f\"Total words evaluated: {total_words}\")\n",
    "print(f\"Correct truth value predictions: {correct_truths}\")\n",
    "print(f\"Proportion of correct predictions: {correct_truths / total_words if total_words > 0 else 0:.2f}\")\n",
    "# Proportion of IDs where all or almost all of the truth value predictions are correct. Pick an informative notion of\n",
    "#   almost all, such as 80%.\n",
    "print(f\"Total IDs evaluated: {total_ids}\")\n",
    "print(f\"IDs with >= 85% correct predictions: {ids_over_85_percent}\")\n",
    "print(f\"Proportion of IDs with >= 85% correct predictions: {ids_over_85_percent / total_ids if total_ids > 0 else 0:.2f}\")\n",
    "# Some other statistic that you find interesting.\n",
    "# \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0f14b3c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
